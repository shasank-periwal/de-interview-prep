{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect | sequence | between\n",
    "recursive dates generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: master\n",
      "25/03/25 16:52:41 WARN Utils: Your hostname, shasankperiwal resolves to a loopback address: 127.0.1.1; using 192.168.20.45 instead (on interface wlp0s20f3)\n",
      "25/03/25 16:52:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/25 16:52:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkSession.builder.appName(name=\"master\").config(\"master\", \"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----------+-------------------+\n",
      "|product_id|period_start|period_end|average_daily_sales|\n",
      "+----------+------------+----------+-------------------+\n",
      "|         1|  2019-01-25|2019-02-28|                100|\n",
      "|         2|  2018-12-01|2020-01-01|                 10|\n",
      "|         3|  2019-12-01|2020-01-31|                  1|\n",
      "+----------+------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType()),\n",
    "    StructField(\"period_start\", DateType()),\n",
    "    StructField(\"period_end\", DateType()),\n",
    "    StructField(\"average_daily_sales\", IntegerType()),\n",
    "])\n",
    "\n",
    "data = [\n",
    "    (1,datetime.strptime('2019-01-25','%Y-%m-%d').date(),datetime.strptime('2019-02-28','%Y-%m-%d').date(),100),\n",
    "    (2,datetime.strptime('2018-12-01','%Y-%m-%d').date(),datetime.strptime('2020-01-01','%Y-%m-%d').date(),10),\n",
    "    (3,datetime.strptime('2019-12-01','%Y-%m-%d').date(),datetime.strptime('2020-01-31','%Y-%m-%d').date(),1)\n",
    "]\n",
    "\n",
    "df = sc.createDataFrame(schema=schema, data=data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find yearly sales of each product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-01\n",
      "2020-01-31\n"
     ]
    }
   ],
   "source": [
    "start_date = df.select(\"period_start\").agg(min(col(\"period_start\"))).collect()[0][0]\n",
    "end_date = df.select(\"period_end\").agg(max(col(\"period_end\"))).collect()[0][0]\n",
    "print(start_date)\n",
    "print(end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|start_date|  end_date|\n",
      "+----------+----------+\n",
      "|2018-12-01|2020-01-31|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dates_df = sc.createDataFrame([(start_date, end_date)], [\"start_date\", \"end_date\"])\n",
    "dates_df.show()\n",
    "all_dates_df = dates_df.select(explode(sequence(col(\"start_date\"), col(\"end_date\"))).alias(\"start_date\"))\n",
    "# all_dates_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------------------------------+\n",
      "|product_id|year|avg_sales_per_product_per_year|\n",
      "+----------+----+------------------------------+\n",
      "|         1|2019|                          3500|\n",
      "|         2|2018|                           310|\n",
      "|         2|2019|                          3650|\n",
      "|         2|2020|                            10|\n",
      "|         3|2019|                            31|\n",
      "|         3|2020|                            31|\n",
      "+----------+----+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = all_dates_df.alias(\"a\").join(df.alias(\"b\"), how=\"left\", on=(col(\"a.start_date\").between(col(\"b.period_start\"), col(\"b.period_end\"))))\\\n",
    "                .select(col(\"product_id\"), year(col(\"start_date\")).alias(\"year\"), col(\"average_daily_sales\"))\\\n",
    "                .groupBy(\"product_id\", \"year\").agg(sum(\"average_daily_sales\").alias(\"avg_sales_per_product_per_year\"))\\\n",
    "                .orderBy(\"product_id\", \"year\")\\\n",
    "                .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
